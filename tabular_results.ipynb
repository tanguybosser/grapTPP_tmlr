{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot.tabulars import get_test_loss_results_v4\n",
    "\n",
    "   \n",
    "models_thp = [\n",
    "    'gru_thp_temporal_with_labels',\n",
    "    'gru_thp-jd_temporal_with_labels',\n",
    "    'gru_temporal_with_labels_gru_temporal_with_labels_thp-dd'\n",
    "]\n",
    "\n",
    "models_sa_thp = [\n",
    "    'selfattention_thp_temporal_with_labels',\n",
    "    'selfattention_thp-jd_temporal_with_labels',\n",
    "    'selfattention_temporal_with_labels_selfattention_temporal_with_labels_thp-dd'\n",
    "]\n",
    "\n",
    "\n",
    "models_sahp = [\n",
    "    'gru_sahp_temporal_with_labels',\n",
    "    'gru_sahp-jd_temporal_with_labels',\n",
    "    'gru_temporal_with_labels_gru_temporal_with_labels_sahp-dd'\n",
    "]\n",
    "\n",
    "models_sa_sahp = [\n",
    "    #Waiting for remaining runs \n",
    "    'selfattention_sahp_temporal_with_labels',\n",
    "    'selfattention_sahp-jd_temporal_with_labels',\n",
    "    #'gru_temporal_with_labels_gru_temporal_with_labels_sep-sahp-mix_separate_ind',\n",
    "    'selfattention_temporal_with_labels_selfattention_temporal_with_labels_sahp-dd'\n",
    "]\n",
    "\n",
    "models_fnn = [\n",
    "    'gru_mlp-cm_temporal_with_labels',\n",
    "    'gru_mlp-cm-jd_temporal_with_labels',\n",
    "    'gru_temporal_with_labels_gru_temporal_with_labels_mlp-cm-dd',\n",
    "]\n",
    "\n",
    "models_sa_fnn = [\n",
    "    'poisson_selfattention_mlp-cm_temporal_with_labels',\n",
    "    'poisson_selfattention_mlp-cm-jd_temporal_with_labels',\n",
    "    #'gru_temporal_with_labels_gru_temporal_with_labels_sep-mlp-cm-mix_separate_ind_base',\n",
    "    'poisson_selfattention_temporal_with_labels_selfattention_temporal_with_labels_mlp-cm-dd',\n",
    "]\n",
    "\n",
    "models_lnm = [\n",
    "    #'gru_log-normal-mixture_temporal_with_labels',\n",
    "    'gru_log-normal-mixture-jd_temporal_with_labels',\n",
    "    #'gru_temporal_with_labels_gru_temporal_with_labels_log-normal-mixture-dd',\n",
    "    'gru_joint-log-normal-mixture_temporal_with_labels'\n",
    "]\n",
    "\n",
    "models_sa_lnm = [\n",
    "    'selfattention_log-normal-mixture_temporal_with_labels_adjust_param',\n",
    "    'selfattention_log-normal-mixture-jd_temporal_with_labels_adjust_param',\n",
    "    'selfattention_temporal_with_labels_selfattention_temporal_with_labels_log-normal-mixture-dd_separate'\n",
    "]\n",
    "\n",
    "models_lnm_joint = [\n",
    "    #'gru_log-normal-mixture-jd_temporal_with_labels_adjust_param',\n",
    "    'gru_joint-log-normal-mixture_temporal_with_labels'\n",
    "]\n",
    "\n",
    "models_rmtpp = [\n",
    "    'gru_rmtpp_temporal_with_labels',\n",
    "    'gru_rmtpp-jd_temporal_with_labels',\n",
    "    'gru_temporal_with_labels_gru_temporal_with_labels_rmtpp-dd',\n",
    "]\n",
    "\n",
    "models_sa_rmtpp = [\n",
    "    'selfattention_rmtpp_temporal_with_labels_adjust_param',\n",
    "    'selfattention_rmtpp-jd_temporal_with_labels_adjust_param',\n",
    "    'selfattention_temporal_with_labels_selfattention_temporal_with_labels_rmtpp-dd_separate',\n",
    "]\n",
    "\n",
    "models_hawkes = [\n",
    "    'stub_hawkes_times_only_check_evaluation'\n",
    "]\n",
    "\n",
    "models_smurf_thp = [\n",
    "    'gru_smurf-thp-jd_temporal_with_labels',\n",
    "    'gru_temporal_with_labels_gru_temporal_with_labels_smurf-thp-dd'\n",
    "]\n",
    "\n",
    "#datasets_marked = ['lastfm_filtered', 'mooc_filtered', 'github_filtered',  'retweets_filtered_short', 'reddit_filtered_short']\n",
    "datasets_marked = [\n",
    "                    'lastfm_filtered',\n",
    "                    'mooc_filtered' , \n",
    "                    'github_filtered', \n",
    "                    'reddit_filtered_short', \n",
    "                    'stack_overflow_filtered',\n",
    "                    #'wikipedia_filtered',\n",
    "                    #'hawkes_exponential_mutual_bis',\n",
    "                    #\"amazon_toys\"\n",
    "                   ]\n",
    "#datasets_marked = ['lastfm_filtered']\n",
    "\n",
    "#models = models_thp + models_sahp + models_lnm + models_rmtpp\n",
    "#models = models_fnn\n",
    "models = models_lnm \n",
    "#models = models_smurf_thp\n",
    "#models = models_sa_thp +models_sa_sahp + models_sa_lnm + models_sa_rmtpp + models_sa_fnn\n",
    "#models = models_thp + models_smurf_thp + models_sahp + models_lnm + models_fnn + models_rmtpp\n",
    "results_dir = 'results/neurips2'\n",
    "\n",
    "get_test_loss_results_v4(results_dir, datasets_marked, models, result_type='marked', n_s=3, to_rank=False, per_dataset=True, std=True)\n",
    "\n",
    "#'retweets_filtered_short', 'reddit_filtered_short'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np \n",
    "\n",
    "def show_results(model_name, splits):\n",
    "    all_marked, all_ground = [], []\n",
    "    for split in range(splits):\n",
    "        path = f'results/simulations2/hawkes_exponential_mutual_large_{model_name}_split{split}.txt'\n",
    "        with open(path, 'rb') as f:\n",
    "            results = pkl.load(f)\n",
    "        all_marked.append(results['marked'])\n",
    "        all_ground.append(results['ground'])\n",
    "    print(model_name)\n",
    "    all_marked = np.concatenate([all_marked], axis=0)\n",
    "    #print(all_marked)\n",
    "    print(f'Marked : {np.mean(all_marked, axis=0)}')\n",
    "    #print(np.sum(np.mean(all_marked, axis=0)))\n",
    "    print(f'Ground : {np.mean(all_ground)}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models=[\n",
    "    'gru_thp_temporal_with_labels',\n",
    "    'gru_thp-jd_temporal_with_labels',\n",
    "    #'gru_temporal_with_labels_gru_temporal_with_labels_thp-dd',\n",
    "\n",
    "    'gru_sahp_temporal_with_labels',\n",
    "    'gru_sahp-jd_temporal_with_labels',\n",
    "    #'gru_temporal_with_labels_gru_temporal_with_labels_sahp-dd',\n",
    "\n",
    "    'gru_mlp-cm_temporal_with_labels',\n",
    "    'gru_mlp-cm-jd_temporal_with_labels',\n",
    "    #'gru_temporal_with_labels_gru_temporal_with_labels_mlp-cm-dd',\n",
    "\n",
    "    'gru_log-normal-mixture_temporal_with_labels',\n",
    "    'gru_log-normal-mixture-jd_temporal_with_labels',\n",
    "    #'gru_temporal_with_labels_gru_temporal_with_labels_log-normal-mixture-dd',\n",
    "\n",
    "    'gru_rmtpp_temporal_with_labels',\n",
    "    'gru_rmtpp-jd_temporal_with_labels',\n",
    "    #'gru_temporal_with_labels_gru_temporal_with_labels_rmtpp-dd'\n",
    "]\n",
    "for model in models:\n",
    "    show_results(model, splits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np \n",
    "from plot.acronyms import get_acronym, map_dataset_name\n",
    "import pandas as pd \n",
    "\n",
    "def get_conflict_tabular(results_dir, datasets, models, splits=1):\n",
    "    n_m = len(models) \n",
    "    all_dic_dataset = dict.fromkeys(datasets)\n",
    "    layers = [\n",
    "            'encoder',\n",
    "            'decoder'\n",
    "                ]\n",
    "    metrics = [\n",
    "        'CG',\n",
    "        'GMS'\n",
    "        ]\n",
    "    for key in all_dic_dataset.keys():\n",
    "        all_dic_dataset[key] = {'encoder': {'CG':[[] for s in range(n_m)], 'GMS':[[] for s in range(n_m)]}, \n",
    "                                'decoder': {'CG':[[] for s in range(n_m)], 'GMS':[[] for s in range(n_m)]}\n",
    "                                }\n",
    "    #print(all_dic_dataset)\n",
    "    for d, dataset in enumerate(datasets):\n",
    "        for m, model in enumerate(models):\n",
    "            for split in range(splits):\n",
    "                file = f'{results_dir}/{dataset}/{dataset}_gru_{model}_temporal_with_labels_split{split}.txt'\n",
    "                with open(file, 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                cos = data['train'][-2]['dot_products']\n",
    "                sim = data['train'][-2]['grad_sim']\n",
    "                \n",
    "                cos_per_layer = {'encoder':[], 'decoder':[]}\n",
    "                sim_per_layer = {'encoder':[], 'decoder':[]}\n",
    "                for name, dot in cos.items():\n",
    "                    if 'encoder' in name:\n",
    "                        cos_per_layer['encoder'].extend(dot)\n",
    "                        sim_per_layer['encoder'].extend(sim[name])\n",
    "                    else:\n",
    "                        cos_per_layer['decoder'].extend(dot)\n",
    "                        sim_per_layer['decoder'].extend(sim[name])\n",
    "                for i, layer in enumerate(layers):    \n",
    "                    dot = np.array(cos_per_layer[layer])\n",
    "                    sim = np.array(sim_per_layer[layer])\n",
    "        \n",
    "                    neg_dot_mask = dot < 0\n",
    "                    pos_sim = sim[~neg_dot_mask]\n",
    "                    neg_sim = sim[neg_dot_mask]\n",
    "                    \n",
    "                    pos_sim = np.round(np.mean(pos_sim),2)\n",
    "                    neg_sim = np.round(np.mean(neg_sim),2)\n",
    "\n",
    "                    prop_pos = np.round((dot > 0).sum()/len(dot),2)\n",
    "                    prop_neg = np.round((dot < 0).sum()/len(dot),2)\n",
    "                    all_dic_dataset[dataset][layer]['CG'][m].append(prop_neg)\n",
    "                    all_dic_dataset[dataset][layer]['GMS'][m].append(neg_sim)\n",
    "    for dataset in datasets:\n",
    "        for layer in layers:\n",
    "            for metric in metrics:\n",
    "                for m in range(n_m):\n",
    "                    all_dic_dataset[dataset][layer][metric][m] = np.round(np.mean(all_dic_dataset[dataset][layer][metric][m]),2)\n",
    "    print(all_dic_dataset)\n",
    "    num_cols = 4 * len(datasets)    \n",
    "    cols = dict.fromkeys(range(num_cols))\n",
    "    d = 0\n",
    "    for dataset, dic_dataset in all_dic_dataset.items():\n",
    "        for layer, dic_layer in dic_dataset.items():\n",
    "            for metric, dic_metric in dic_layer.items():\n",
    "                #print(all_dic_dataset[dataset][layer][metric])\n",
    "                cols[d] = all_dic_dataset[dataset][layer][metric]\n",
    "                d += 1\n",
    "    df = pd.DataFrame(cols)\n",
    "    print(df.to_latex(index=False))\n",
    "    \n",
    "    \n",
    "def show_cg_sim(results_dir, models, datasets, splits):\n",
    "    for d, dataset in enumerate(datasets):\n",
    "        dataset_name = map_dataset_name(dataset)\n",
    "        print(dataset_name)\n",
    "        for m, model in enumerate(models):\n",
    "            all_grad, all_sim = cos_per_layer = {'encoder':[], 'decoder':[]}, {'encoder':[], 'decoder':[]}\n",
    "            model_name = f'gru_{model}_temporal_with_labels'\n",
    "            model_name = get_acronym([model_name])[0]\n",
    "            print(f'{model_name}\\n')\n",
    "            for split in range(splits):\n",
    "                file = f'{results_dir}/{dataset}/{dataset}_gru_{model}_temporal_with_labels_split{split}.txt'\n",
    "                with open(file, 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                cos = data['train'][-2]['dot_products']\n",
    "                sim = data['train'][-2]['grad_sim']\n",
    "                layers = [\n",
    "                    'encoder',\n",
    "                    'decoder'\n",
    "                ]\n",
    "                cos_per_layer = {'encoder':[], 'decoder':[]}\n",
    "                sim_per_layer = {'encoder':[], 'decoder':[]}\n",
    "                for name, dot in cos.items():\n",
    "                    if 'encoder' in name:\n",
    "                        cos_per_layer['encoder'].extend(dot)\n",
    "                        sim_per_layer['encoder'].extend(sim[name])\n",
    "                    else:\n",
    "                        cos_per_layer['decoder'].extend(dot)\n",
    "                        sim_per_layer['decoder'].extend(sim[name])\n",
    "                for i, layer in enumerate(layers):    \n",
    "                    dot = np.array(cos_per_layer[layer])\n",
    "                    sim = np.array(sim_per_layer[layer])\n",
    "        \n",
    "                    neg_dot_mask = dot < 0\n",
    "                    #pos_sim = sim[~neg_dot_mask]\n",
    "                    neg_sim = sim[neg_dot_mask]\n",
    "                    \n",
    "                    #pos_sim = np.round(np.mean(pos_sim),2)\n",
    "                    neg_sim = np.round(np.mean(neg_sim),2)\n",
    "\n",
    "                    #prop_pos = np.round((dot > 0).sum()/len(dot),2)\n",
    "                    prop_neg = np.round((dot < 0).sum()/len(dot),2)\n",
    "                    all_grad[layer].append(prop_neg)\n",
    "                    all_sim[layer].append(neg_sim)\n",
    "            for layer in layers:\n",
    "                grad = np.round(np.mean(all_grad[layer]), 2)\n",
    "                sim = np.round(np.mean(all_sim[layer]), 2)\n",
    "                print(f'{layer}\\nCG:{grad}\\nGMS:{sim}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    'lastfm_filtered',\n",
    "    'mooc_filtered',\n",
    "    'github_filtered',\n",
    "    'reddit_filtered_short',\n",
    "    'stack_overflow_filtered',\n",
    "    #'hawkes_sum_exponential_mutual',\n",
    "    ]\n",
    "\n",
    "\n",
    "models = [\n",
    "    'thp',\n",
    "    'thp-jd',\n",
    "    'sahp',\n",
    "    'sahp-jd',\n",
    "    'mlp-cm',\n",
    "    'mlp-cm-jd', \n",
    "    'log-normal-mixture',\n",
    "    'log-normal-mixture-jd',\n",
    "    'rmtpp',\n",
    "    'rmtpp-jd'\n",
    "]\n",
    "\n",
    "splits = 3\n",
    "\n",
    "results_dir = 'results/neurips3'\n",
    "\n",
    "#show_cg_sim(results_dir, models, datasets, splits)\n",
    "get_conflict_tabular(results_dir, datasets, models, splits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanguy/miniconda/envs/env_tpp/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/tanguy/miniconda/envs/env_tpp/lib/python3.9/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thp\n",
      "Mean:5.19, Std.:0.03\n",
      "thp-jd\n",
      "Mean:4.15, Std.:0.06\n",
      "thp-dd\n",
      "Mean:5.76, Std.:0.08\n",
      "sahp\n",
      "Mean:8.08, Std.:0.07\n",
      "sahp-jd\n",
      "Mean:5.91, Std.:0.06\n",
      "sahp-dd\n",
      "Mean:23.49, Std.:0.21\n",
      "mlp-cm\n",
      "Mean:6.36, Std.:0.07\n",
      "mlp-cm-jd\n",
      "Mean:6.28, Std.:0.08\n",
      "mlp-cm-dd\n",
      "Mean:8.07, Std.:0.09\n",
      "log-normal-mixture\n",
      "Mean:3.97, Std.:0.02\n",
      "log-normal-mixture-jd\n",
      "Mean:3.99, Std.:0.03\n",
      "log-normal-mixture-dd\n",
      "Mean:5.61, Std.:0.01\n",
      "rmtpp\n",
      "Mean:3.24, Std.:0.01\n",
      "rmtpp-jd\n",
      "Mean:3.33, Std.:0.0\n",
      "rmtpp-dd\n",
      "Mean:4.37, Std.:0.07\n",
      "smurf-thp-jd\n",
      "Mean:5.61, Std.:0.04\n",
      "smurf-thp-dd\n",
      "Mean:6.72, Std.:0.1\n"
     ]
    }
   ],
   "source": [
    "from plot.plots_esann import computational_time\n",
    "\n",
    "result_dir = 'results/comput'\n",
    "dataset = 'mooc_filtered'\n",
    "models = [\n",
    "    'thp',\n",
    "    'sahp',\n",
    "    'mlp-cm',\n",
    "    'log-normal-mixture',\n",
    "    'rmtpp',\n",
    "    'smurf-thp'\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for model in models:\n",
    "    computational_time(result_dir, model, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_tpp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
