#!/bin/bash
  
#SBATCH --job-name=lnm_joint
#SBATCH --output=work_outputs/lnm_joint.out

#SBATCH --cpus-per-task=10
#SBATCH --mem=10G
#SBATCH --gres=gpu:1


##Lastfm

for split in {3..4}
do
python3 -u scripts/train2.py --no-mlflow --dataset 'lastfm_filtered' --load-from-dir '../neuralTPPs/data/baseline3' \
--save-results-dir 'results/icml/lastfm_filtered' --save-check-dir 'checkpoints/icml/lastfm_filtered' \
--eval-metrics True --include-poisson False --patience 100 --batch-size 8 --split $split \
--encoder 'gru' --encoder-encoding 'temporal_with_labels' --encoder-emb-dim 8 \
--encoder-units-rnn 42 --encoder-layers-rnn 1 \
--encoder-units-mlp 32 --encoder-activation-mlp 'relu' \
--decoder 'joint-log-normal-mixture' \
--decoder-units-mlp 32 --decoder-units-mlp 16 \
--decoder-n-mixture 32 \
--exp-name 'adjust_param'
done



#MOOC

for split in {3..4}
do
python3 -u scripts/train2.py --no-mlflow --dataset 'mooc_filtered' --load-from-dir '../neuralTPPs/data/baseline3' \
--save-results-dir 'results/icml/mooc_filtered' --save-check-dir 'checkpoints/icml/mooc_filtered' \
--eval-metrics True --include-poisson False --patience 100 --batch-size 32 --split $split \
--encoder 'gru' --encoder-encoding 'temporal_with_labels' --encoder-emb-dim 8 \
--encoder-units-rnn 42 --encoder-layers-rnn 1 \
--encoder-units-mlp 32 --encoder-activation-mlp 'relu' \
--decoder 'joint-log-normal-mixture' \
--decoder-units-mlp 32 --decoder-units-mlp 16 \
--decoder-n-mixture 32 \
--exp-name 'adjust_param'
done


#Github

for split in {3..4}
do
python3 -u scripts/train2.py --no-mlflow --dataset 'github_filtered' --load-from-dir '../neuralTPPs/data/baseline3' \
--save-results-dir 'results/icml/github_filtered' --save-check-dir 'checkpoints/icml/github_filtered' \
--eval-metrics True --include-poisson False --patience 100 --batch-size 8 --split $split \
--encoder 'gru' --encoder-encoding 'temporal_with_labels' --encoder-emb-dim 8 \
--encoder-units-rnn 42 --encoder-layers-rnn 1 \
--encoder-units-mlp 32 --encoder-activation-mlp 'relu' \
--decoder 'joint-log-normal-mixture' \
--decoder-units-mlp 32 --decoder-units-mlp 16 \
--decoder-n-mixture 32 \
--exp-name 'adjust_param'
done


#Reddit

for split in {3..4}
do
python3 -u scripts/train2.py --no-mlflow --dataset 'reddit_filtered_short' --load-from-dir '../neuralTPPs/data/baseline3' \
--save-results-dir 'results/icml/reddit_filtered_short' --save-check-dir 'checkpoints/icml/reddit_filtered_short' \
--eval-metrics True --include-poisson False --patience 100 --batch-size 8 --split $split \
--encoder 'gru' --encoder-encoding 'temporal_with_labels' --encoder-emb-dim 8 \
--encoder-units-rnn 42 --encoder-layers-rnn 1 \
--encoder-units-mlp 32 --encoder-activation-mlp 'relu' \
--decoder 'joint-log-normal-mixture' \
--decoder-units-mlp 32 --decoder-units-mlp 16 \
--decoder-n-mixture 32 \
--exp-name 'adjust_param'
done

#Retweets

for split in {3..4}
do
python3 -u scripts/train2.py --no-mlflow --dataset 'retweets_filtered_short' --load-from-dir '../neuralTPPs/data/baseline3' \
--save-results-dir 'results/icml/retweets_filtered_short' --save-check-dir 'checkpoints/icml/retweets_filtered_short' \
--eval-metrics True --include-poisson False --patience 100 --batch-size 8 --split $split \
--encoder 'gru' --encoder-encoding 'temporal_with_labels' --encoder-emb-dim 8 \
--encoder-units-rnn 42 --encoder-layers-rnn 1 \
--encoder-units-mlp 32 --encoder-activation-mlp 'relu' \
--decoder 'joint-log-normal-mixture' \
--decoder-units-mlp 32 --decoder-units-mlp 16 \
--decoder-n-mixture 32 \
--exp-name 'adjust_param'
done

#Stack Overflow

for split in {3..4}
do
python3 -u scripts/train2.py --no-mlflow --dataset 'stack_overflow_filtered' --load-from-dir '../neuralTPPs/data/baseline3' \
--save-results-dir 'results/icml/stack_overflow_filtered' --save-check-dir 'checkpoints/icml/stack_overflow_filtered' \
--eval-metrics True --include-poisson False --patience 100 --batch-size 8 --split $split \
--encoder 'gru' --encoder-encoding 'temporal_with_labels' --encoder-emb-dim 8 \
--encoder-units-rnn 42 --encoder-layers-rnn 1 \
--encoder-units-mlp 32 --encoder-activation-mlp 'relu' \
--decoder 'joint-log-normal-mixture' \
--decoder-units-mlp 32 --decoder-units-mlp 16 \
--decoder-n-mixture 32 \
--exp-name 'adjust_param'
done


: "
#Hawkes

for split in 0
do
python3 -u scripts/train2.py --no-mlflow --dataset 'hawkes_exponential_mutual' --load-from-dir '../neuralTPPs/data/baseline3' \
--save-results-dir 'results/icml/hawkes_exponential_mutual' --save-check-dir 'checkpoints/icml/hawkes_exponential_mutual' \
--eval-metrics True --include-poisson False --patience 100 --batch-size 8 --split $split \
--encoder 'gru' --encoder-encoding 'temporal_with_labels' --encoder-emb-dim 8 \
--encoder-units-rnn 42 --encoder-layers-rnn 1 \
--encoder-units-mlp 32 --encoder-activation-mlp 'relu' \
--decoder 'joint-log-normal-mixture' \
--decoder-units-mlp 32 --decoder-units-mlp 16 \
--decoder-n-mixture 32 \
--exp-name 'adjust_param'
done
"