#!/bin/bash
  
#SBATCH --job-name=hawkes
#SBATCH --output=work_outputs/hawkes.out

#SBATCH --cpus-per-task=10
#SBATCH --mem=10G
#SBATCH --gres=gpu:1

for split in 0
do
python3 -u scripts/train2.py --no-mlflow --dataset 'hawkes_exponential_mutual' --load-from-dir '../neuralTPPs/data/baseline3' \
--save-results-dir 'results/icml/hawkes_exponential_mutual' --save-check-dir 'checkpoints/icml/hawkes_exponential_mutual' \
--eval-metrics True --include-poisson False --patience 100 --batch-size 8 --split $split \
--encoder 'stub' --decoder 'hawkes'
done
