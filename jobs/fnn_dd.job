#!/bin/bash
  
#SBATCH --job-name=fnn_dd
#SBATCH --output=neurips_output/fnn_dd_hawkes.out

#SBATCH --cpus-per-task=10
#SBATCH --mem=10G
#SBATCH --gres=gpu:1

: "
##LastFM

for split in {0..2}
do
python3 -u scripts/train2.py --no-mlflow --dataset 'lastfm_filtered' --load-from-dir '../neuralTPPs/data/baseline3' \
--save-results-dir 'results/neurips2/lastfm_filtered' --save-check-dir 'checkpoints/neurips2/lastfm_filtered' \
--eval-metrics True --include-poisson False --patience 50 --batch-size 8 --split $split \
--encoder-histtime 'gru' --encoder-histmark 'gru' \
--encoder-histtime-encoding 'temporal_with_labels' --encoder-histmark-encoding 'temporal_with_labels' \
--encoder-emb-dim 8 --encoder-embedding-constraint 'nonneg' \
--encoder-units-rnn 30 --encoder-layers-rnn 1 --encoder-constraint-rnn 'nonneg' \
--encoder-units-mlp 16 --encoder-activation-mlp 'relu' --encoder-constraint-mlp 'nonneg' \
--decoder 'mlp-cm-dd' --decoder-encoding 'log_times_only' --decoder-emb-dim 8 --decoder-embedding-constraint 'nonneg' \
--decoder-units-mlp 32 --decoder-units-mlp 32 --decoder-activation-mlp 'gumbel_softplus' \
--decoder-activation-final-mlp 'parametric_softplus' --decoder-constraint-mlp 'nonneg' \
--decoder-hist-time-grouping 'concatenation' \
--separate-training True \
--train-epochs 50
done 

##MOOC

for split in {0..2}
do
python3 -u scripts/train2.py --no-mlflow --dataset 'mooc_filtered' --load-from-dir '../neuralTPPs/data/baseline3' \
--save-results-dir 'results/neurips2/mooc_filtered' --save-check-dir 'checkpoints/neurips2/mooc_filtered' \
--eval-metrics True --include-poisson False --patience 50 --batch-size 32 --split $split \
--encoder-histtime 'gru' --encoder-histmark 'gru' \
--encoder-histtime-encoding 'temporal_with_labels' --encoder-histmark-encoding 'temporal_with_labels' \
--encoder-emb-dim 8 --encoder-embedding-constraint 'nonneg' \
--encoder-units-rnn 30 --encoder-layers-rnn 1 --encoder-constraint-rnn 'nonneg' \
--encoder-units-mlp 16 --encoder-activation-mlp 'relu' --encoder-constraint-mlp 'nonneg' \
--decoder 'mlp-cm-dd' --decoder-encoding 'log_times_only' --decoder-emb-dim 8 --decoder-embedding-constraint 'nonneg' \
--decoder-units-mlp 32 --decoder-units-mlp 32 --decoder-activation-mlp 'gumbel_softplus' \
--decoder-activation-final-mlp 'parametric_softplus' --decoder-constraint-mlp 'nonneg' \
--decoder-hist-time-grouping 'concatenation' \
--separate-training True
done 

##Stack

for split in {0..2}
do
python3 -u scripts/train2.py --no-mlflow --dataset 'stack_overflow_filtered' --load-from-dir '../neuralTPPs/data/baseline3' \
--save-results-dir 'results/neurips2/stack_overflow_filtered' --save-check-dir 'checkpoints/neurips2/stack_overflow_filtered' \
--eval-metrics True --include-poisson False --patience 50 --batch-size 8 --split $split \
--encoder-histtime 'gru' --encoder-histmark 'gru' \
--encoder-histtime-encoding 'temporal_with_labels' --encoder-histmark-encoding 'temporal_with_labels' \
--encoder-emb-dim 8 --encoder-embedding-constraint 'nonneg' \
--encoder-units-rnn 30 --encoder-layers-rnn 1 --encoder-constraint-rnn 'nonneg' \
--encoder-units-mlp 16 --encoder-activation-mlp 'relu' --encoder-constraint-mlp 'nonneg' \
--decoder 'mlp-cm-dd' --decoder-encoding 'log_times_only' --decoder-emb-dim 8 --decoder-embedding-constraint 'nonneg' \
--decoder-units-mlp 32 --decoder-units-mlp 32 --decoder-activation-mlp 'gumbel_softplus' \
--decoder-activation-final-mlp 'parametric_softplus' --decoder-constraint-mlp 'nonneg' \
--decoder-hist-time-grouping 'concatenation' \
--separate-training True
done 

##Github

for split in {0..2}
do
python3 -u scripts/train2.py --no-mlflow --dataset 'github_filtered' --load-from-dir '../neuralTPPs/data/baseline3' \
--save-results-dir 'results/neurips2/github_filtered' --save-check-dir 'checkpoints/neurips2/github_filtered' \
--eval-metrics True --include-poisson False --patience 50 --batch-size 8 --split $split \
--encoder-histtime 'gru' --encoder-histmark 'gru' \
--encoder-histtime-encoding 'temporal_with_labels' --encoder-histmark-encoding 'temporal_with_labels' \
--encoder-emb-dim 8 --encoder-embedding-constraint 'nonneg' \
--encoder-units-rnn 30 --encoder-layers-rnn 1 --encoder-constraint-rnn 'nonneg' \
--encoder-units-mlp 16 --encoder-activation-mlp 'relu' --encoder-constraint-mlp 'nonneg' \
--decoder 'mlp-cm-dd' --decoder-encoding 'log_times_only' --decoder-emb-dim 8 --decoder-embedding-constraint 'nonneg' \
--decoder-units-mlp 32 --decoder-units-mlp 32 --decoder-activation-mlp 'gumbel_softplus' \
--decoder-activation-final-mlp 'parametric_softplus' --decoder-constraint-mlp 'nonneg' \
--decoder-hist-time-grouping 'concatenation' \
--separate-training True
done 

##Reddit

for split in {0..2}
do
python3 -u scripts/train2.py --no-mlflow --dataset 'reddit_filtered_short' --load-from-dir '../neuralTPPs/data/baseline3' \
--save-results-dir 'results/neurips2/reddit_filtered_short' --save-check-dir 'checkpoints/neurips2/reddit_filtered_short' \
--eval-metrics True --include-poisson False --patience 50 --batch-size 8 --split $split \
--encoder-histtime 'gru' --encoder-histmark 'gru' \
--encoder-histtime-encoding 'temporal_with_labels' --encoder-histmark-encoding 'temporal_with_labels' \
--encoder-emb-dim 8 --encoder-embedding-constraint 'nonneg' \
--encoder-units-rnn 30 --encoder-layers-rnn 1 --encoder-constraint-rnn 'nonneg' \
--encoder-units-mlp 16 --encoder-activation-mlp 'relu' --encoder-constraint-mlp 'nonneg' \
--decoder 'mlp-cm-dd' --decoder-encoding 'log_times_only' --decoder-emb-dim 8 --decoder-embedding-constraint 'nonneg' \
--decoder-units-mlp 32 --decoder-units-mlp 32 --decoder-activation-mlp 'gumbel_softplus' \
--decoder-activation-final-mlp 'parametric_softplus' --decoder-constraint-mlp 'nonneg' \
--decoder-hist-time-grouping 'concatenation' \
--separate-training True
done 
"

for split in 0
do
python3 -u scripts/train2.py --no-mlflow --dataset 'hawkes_exponential_mutual_large' --load-from-dir 'data/baseline3' \
--save-results-dir 'results/neurips2/hawkes_exponential_mutual_large' --save-check-dir 'checkpoints/neurips2/hawkes_exponential_mutual_large' \
--eval-metrics True --include-poisson False --patience 100 --batch-size 16 --split $split \
--encoder-histtime 'gru' --encoder-histmark 'gru' \
--encoder-histtime-encoding 'temporal_with_labels' --encoder-histmark-encoding 'temporal_with_labels' \
--encoder-emb-dim 8 --encoder-embedding-constraint 'nonneg' \
--encoder-units-rnn 30 --encoder-layers-rnn 1 --encoder-constraint-rnn 'nonneg' \
--encoder-units-mlp 16 --encoder-activation-mlp 'relu' --encoder-constraint-mlp 'nonneg' \
--decoder 'mlp-cm-dd' --decoder-encoding 'log_times_only' --decoder-emb-dim 8 --decoder-embedding-constraint 'nonneg' \
--decoder-units-mlp 32 --decoder-units-mlp 32 --decoder-activation-mlp 'gumbel_softplus' \
--decoder-activation-final-mlp 'parametric_softplus' --decoder-constraint-mlp 'nonneg' \
--decoder-hist-time-grouping 'concatenation' \
--separate-training True \
--loss-relative-tolerance 0
done 



: "
##Wikipedia

for split in {0..2}
do
python3 -u scripts/train2.py --no-mlflow --dataset 'wikipedia_filtered' --load-from-dir '../neuralTPPs/data/baseline3' \
--save-results-dir 'results/neurips2/wikipedia_filtered' --save-check-dir 'checkpoints/neurips2/wikipedia_filtered' \
--eval-metrics True --include-poisson False --patience 50 --batch-size 16 --split $split \
--encoder-histtime 'gru' --encoder-histmark 'gru' \
--encoder-histtime-encoding 'temporal_with_labels' --encoder-histmark-encoding 'temporal_with_labels' \
--encoder-emb-dim 8 --encoder-embedding-constraint 'nonneg' \
--encoder-units-rnn 30 --encoder-layers-rnn 1 --encoder-constraint-rnn 'nonneg' \
--encoder-units-mlp 16 --encoder-activation-mlp 'relu' --encoder-constraint-mlp 'nonneg' \
--decoder 'mlp-cm-dd' --decoder-encoding 'log_times_only' --decoder-emb-dim 8 --decoder-embedding-constraint 'nonneg' \
--decoder-units-mlp 32 --decoder-units-mlp 32 --decoder-activation-mlp 'gumbel_softplus' \
--decoder-activation-final-mlp 'parametric_softplus' --decoder-constraint-mlp 'nonneg' \
--decoder-hist-time-grouping 'concatenation' \
--separate-training True
done 


##Hawkes - Large

for split in {0..2}
do
python3 -u scripts/train2.py --no-mlflow --dataset 'hawkes_exponential_mutual_large' --load-from-dir 'data/baseline3' \
--save-results-dir 'results/neurips2/hawkes_exponential_mutual_large' --save-check-dir 'checkpoints/neurips2/hawkes_exponential_mutual_large' \
--eval-metrics True --include-poisson False --patience 50 --batch-size 16 --split $split \
--encoder-histtime 'gru' --encoder-histmark 'gru' \
--encoder-histtime-encoding 'temporal_with_labels' --encoder-histmark-encoding 'temporal_with_labels' \
--encoder-emb-dim 8 --encoder-embedding-constraint 'nonneg' \
--encoder-units-rnn 30 --encoder-layers-rnn 1 --encoder-constraint-rnn 'nonneg' \
--encoder-units-mlp 16 --encoder-activation-mlp 'relu' --encoder-constraint-mlp 'nonneg' \
--decoder 'mlp-cm-dd' --decoder-encoding 'log_times_only' --decoder-emb-dim 8 --decoder-embedding-constraint 'nonneg' \
--decoder-units-mlp 32 --decoder-units-mlp 32 --decoder-activation-mlp 'gumbel_softplus' \
--decoder-activation-final-mlp 'parametric_softplus' --decoder-constraint-mlp 'nonneg' \
--decoder-hist-time-grouping 'concatenation' \
--separate-training True
done 

##Retweets

for split in {0..2}
do
python3 -u scripts/train2.py --no-mlflow --dataset 'retweets_filtered_short' --load-from-dir '../neuralTPPs/data/baseline3' \
--save-results-dir 'results/neurips2/retweets_filtered_short' --save-check-dir 'checkpoints/neurips2/retweets_filtered_short' \
--eval-metrics True --include-poisson False --patience 50 --batch-size 8 --split $split \
--encoder-histtime 'gru' --encoder-histmark 'gru' \
--encoder-histtime-encoding 'temporal_with_labels' --encoder-histmark-encoding 'temporal_with_labels' \
--encoder-emb-dim 8 --encoder-embedding-constraint 'nonneg' \
--encoder-units-rnn 30 --encoder-layers-rnn 1 --encoder-constraint-rnn 'nonneg' \
--encoder-units-mlp 16 --encoder-activation-mlp 'relu' --encoder-constraint-mlp 'nonneg' \
--decoder 'mlp-cm-dd' --decoder-encoding 'log_times_only' --decoder-emb-dim 8 --decoder-embedding-constraint 'nonneg' \
--decoder-units-mlp 32 --decoder-units-mlp 32 --decoder-activation-mlp 'gumbel_softplus' \
--decoder-activation-final-mlp 'parametric_softplus' --decoder-constraint-mlp 'nonneg' \
--decoder-hist-time-grouping 'concatenation' \
--separate-training True
done 



##Hawkes

for split in 0
do
python3 -u scripts/train2.py --no-mlflow --dataset 'hawkes_exponential_mutual_large' --load-from-dir 'data/baseline3' \
--save-results-dir 'results/neurips2/hawkes_exponential_mutual_large' --save-check-dir 'checkpoints/neurips2/hawkes_exponential_mutual_large' \
--eval-metrics True --include-poisson False --patience 20 --train-epochs 1001 --batch-size 16 --split $split \
--encoder-histtime 'gru' --encoder-histmark 'gru' \
--encoder-histtime-encoding 'temporal_with_labels' --encoder-histmark-encoding 'temporal_with_labels' \
--encoder-emb-dim 8 --encoder-embedding-constraint 'nonneg' \
--encoder-units-rnn 30 --encoder-layers-rnn 1 --encoder-constraint-rnn 'nonneg' \
--encoder-units-mlp 16 --encoder-activation-mlp 'relu' --encoder-constraint-mlp 'nonneg' \
--decoder 'mlp-cm-dd' --decoder-encoding 'log_times_only' --decoder-emb-dim 8 --decoder-embedding-constraint 'nonneg' \
--decoder-units-mlp 32 --decoder-units-mlp 32 --decoder-activation-mlp 'gumbel_softplus' \
--decoder-activation-final-mlp 'parametric_softplus' --decoder-constraint-mlp 'nonneg' \
--decoder-hist-time-grouping 'concatenation' \
--separate-training True
done 


## Amazon Movies 

for split in {0..2}
do
python3 -u scripts/train2.py --no-mlflow --dataset 'amazon_movies' --load-from-dir 'data/baseline3' \
--save-results-dir 'results/neurips2/amazon_movies' --save-check-dir 'checkpoints/neurips2/amazon_movies' \
--eval-metrics True --include-poisson False --patience 50 --batch-size 8 --split $split \
--encoder-histtime 'gru' --encoder-histmark 'gru' \
--encoder-histtime-encoding 'temporal_with_labels' --encoder-histmark-encoding 'temporal_with_labels' \
--encoder-emb-dim 8 --encoder-embedding-constraint 'nonneg' \
--encoder-units-rnn 30 --encoder-layers-rnn 1 --encoder-constraint-rnn 'nonneg' \
--encoder-units-mlp 16 --encoder-activation-mlp 'relu' --encoder-constraint-mlp 'nonneg' \
--decoder 'mlp-cm-dd' --decoder-encoding 'log_times_only' --decoder-emb-dim 8 --decoder-embedding-constraint 'nonneg' \
--decoder-units-mlp 32 --decoder-units-mlp 32 --decoder-activation-mlp 'gumbel_softplus' \
--decoder-activation-final-mlp 'parametric_softplus' --decoder-constraint-mlp 'nonneg' \
--decoder-hist-time-grouping 'concatenation' \
--separate-training True
done 
"
: "
## Amazon Toys

for split in {0..2}
do
python3 -u scripts/train2.py --no-mlflow --dataset 'amazon_toys' --load-from-dir 'data/baseline3' \
--save-results-dir 'results/neurips2/amazon_toys' --save-check-dir 'checkpoints/neurips2/amazon_toys' \
--eval-metrics True --include-poisson False --patience 50 --batch-size 8 --split $split \
--encoder-histtime 'gru' --encoder-histmark 'gru' \
--encoder-histtime-encoding 'temporal_with_labels' --encoder-histmark-encoding 'temporal_with_labels' \
--encoder-emb-dim 8 --encoder-embedding-constraint 'nonneg' \
--encoder-units-rnn 30 --encoder-layers-rnn 1 --encoder-constraint-rnn 'nonneg' \
--encoder-units-mlp 16 --encoder-activation-mlp 'relu' --encoder-constraint-mlp 'nonneg' \
--decoder 'mlp-cm-dd' --decoder-encoding 'log_times_only' --decoder-emb-dim 8 --decoder-embedding-constraint 'nonneg' \
--decoder-units-mlp 32 --decoder-units-mlp 32 --decoder-activation-mlp 'gumbel_softplus' \
--decoder-activation-final-mlp 'parametric_softplus' --decoder-constraint-mlp 'nonneg' \
--decoder-hist-time-grouping 'concatenation' \
--separate-training True
done 