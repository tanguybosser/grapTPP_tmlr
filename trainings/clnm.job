#!/bin/bash
  
#SBATCH --job-name=clnm_so_nw
#SBATCH --output=outputs/clnm_so_nw.out

#SBATCH --cpus-per-task=10
#SBATCH --mem=10G
#SBATCH --gres=gpu:1

: "
## LastFM

for split in {0..2}
do
python3 -u scripts/train2.py --no-mlflow --dataset 'lastfm' --load-from-dir 'data/baseline3' \
--save-results-dir 'results/neurips/lastfm' --save-check-dir 'checkpoints/neurips/lastfm' \
--eval-metrics True --include-poisson False --patience 50 --batch-size 4 --split $split \
--encoder 'gru' --encoder-encoding 'temporal_with_labels' --encoder-emb-dim 8 \
--encoder-units-rnn 32 --encoder-layers-rnn 1 \
--encoder-units-mlp 16 --encoder-activation-mlp 'relu' \
--decoder 'cond-log-normal-mixture' \
--decoder-units-mlp 16 --decoder-units-mlp 16 \
--decoder-n-mixture 32 \
--decoder-n-mark-mixture 1 \
--decoder-encoding 'log_times_only' \
--decoder-hist-time-grouping 'concatenation'
done 
"
: "
## MOOC

for split in {0..2}
do
python3 -u scripts/train2.py --no-mlflow --dataset 'mooc' --load-from-dir 'data/baseline3' \
--save-results-dir 'results/neurips/mooc' --save-check-dir 'checkpoints/neurips/mooc' \
--eval-metrics True --include-poisson False --patience 50 --batch-size 32 --split $split \
--encoder 'gru' --encoder-encoding 'temporal_with_labels' --encoder-emb-dim 8 \
--encoder-units-rnn 32 --encoder-layers-rnn 1 \
--encoder-units-mlp 16 --encoder-activation-mlp 'relu' \
--decoder 'cond-log-normal-mixture' \
--decoder-units-mlp 16 --decoder-units-mlp 16 \
--decoder-n-mixture 32 \
--decoder-n-mark-mixture 1 \
--decoder-encoding 'log_times_only' \
--decoder-hist-time-grouping 'concatenation' 
done 
"

: "
## Github

for split in {0..2}
do
python3 -u scripts/train2.py --no-mlflow --dataset 'github_filtered' --load-from-dir 'data/baseline3' \
--save-results-dir 'results/neurips/github_filtered' --save-check-dir 'checkpoints/neurips/github_filtered' \
--eval-metrics True --include-poisson False --patience 50 --batch-size 16 --split $split \
--encoder 'gru' --encoder-encoding 'temporal_with_labels' --encoder-emb-dim 8 \
--encoder-units-rnn 32   --encoder-layers-rnn 1 \
--encoder-units-mlp 16 --encoder-activation-mlp 'relu' \
--decoder 'cond-log-normal-mixture' \
--decoder-units-mlp 16 --decoder-units-mlp 16 \
--decoder-n-mixture 32 \
--decoder-n-mark-mixture 1 \
--decoder-encoding 'log_times_only' \
--decoder-hist-time-grouping 'concatenation' 
done
"

: "
## Reddit 

for split in {0..2}
do
python3 -u scripts/train2.py --no-mlflow --dataset 'reddit' --load-from-dir 'data/baseline3' \
--save-results-dir 'results/neurips/reddit' --save-check-dir 'checkpoints/neurips/reddit' \
--eval-metrics True --include-poisson False --patience 50 --batch-size 10 --split $split \
--encoder 'gru' --encoder-encoding 'temporal_with_labels' --encoder-emb-dim 8 \
--encoder-units-rnn 32 --encoder-layers-rnn 1 \
--encoder-units-mlp 16 --encoder-activation-mlp 'relu' \
--decoder 'cond-log-normal-mixture' \
--decoder-units-mlp 16 --decoder-units-mlp 16 \
--decoder-n-mixture 32 \
--decoder-n-mark-mixture 1 \
--decoder-encoding 'log_times_only' \
--decoder-hist-time-grouping 'concatenation' 
done 
"

: "
## Retweets

for split in {0..2}
do
python3 -u scripts/train2.py --no-mlflow --dataset 'retweets' --load-from-dir 'data/baseline3' \
--save-results-dir 'results/neurips/retweets' --save-check-dir 'checkpoints/neurips/retweets' \
--eval-metrics True --include-poisson False --patience 50 --batch-size 8 --split $split \
--encoder 'gru' --encoder-encoding 'temporal_with_labels' --encoder-emb-dim 8 \
--encoder-units-rnn 32 --encoder-layers-rnn 1 \
--encoder-units-mlp 16 --encoder-activation-mlp 'relu' \
--decoder 'cond-log-normal-mixture' \
--decoder-units-mlp 16 --decoder-units-mlp 16 \
--decoder-n-mixture 32 \
--decoder-n-mark-mixture 1 \
--decoder-encoding 'log_times_only' \
--decoder-hist-time-grouping 'concatenation' 
done 
"


## Stack Overflow

: "
for split in {0..2}
do
python3 -u scripts/train2.py --no-mlflow --dataset 'stack_overflow' --load-from-dir 'data/baseline3' \
--save-results-dir 'results/neurips/stack_overflow' --save-check-dir 'checkpoints/neurips/stack_overflow' \
--eval-metrics True --include-poisson False --patience 50 --batch-size 16 --split $split \
--encoder 'gru' --encoder-encoding 'temporal_with_labels' --encoder-emb-dim 8 \
--encoder-units-rnn 32 --encoder-layers-rnn 1 \
--encoder-units-mlp 16 --encoder-activation-mlp 'relu' \
--decoder 'cond-log-normal-mixture' \
--decoder-units-mlp 16 --decoder-units-mlp 16 \
--decoder-n-mixture 32 \
--decoder-n-mark-mixture 1 \
--decoder-encoding 'log_times_only' \
--decoder-hist-time-grouping 'concatenation' 
done 
"

## Stack Overflow - No Window 

for split in {0..2}
do
python3 -u scripts/train2.py --no-mlflow --dataset 'stack_overflow_nowindow' --load-from-dir 'data/baseline3' \
--save-results-dir 'results/neurips/stack_overflow_nowindow' --save-check-dir 'checkpoints/neurips/stack_overflow_nowindow' \
--eval-metrics True --include-poisson False --patience 50 --batch-size 16 --split $split \
--encoder 'gru' --encoder-encoding 'temporal_with_labels' --encoder-emb-dim 8 \
--encoder-units-rnn 32 --encoder-layers-rnn 1 \
--encoder-units-mlp 16 --encoder-activation-mlp 'relu' \
--decoder 'cond-log-normal-mixture' \
--decoder-units-mlp 16 --decoder-units-mlp 16 \
--decoder-n-mixture 32 \
--decoder-n-mark-mixture 1 \
--decoder-encoding 'log_times_only' \
--decoder-hist-time-grouping 'concatenation' 
done 

: "
##Hawkes

for split in {0..2}
do
python3 -u scripts/train2.py --no-mlflow --dataset 'hawkes_exponential_mutual' --load-from-dir 'data/baseline3' \
--save-results-dir 'results/neurips/hawkes_exponential_mutual' --save-check-dir 'checkpoints/neurips/hawkes_exponential_mutual' \
--eval-metrics True --include-poisson False --patience 50 --batch-size 16 --split $split \
--encoder 'gru' --encoder-encoding 'temporal_with_labels' --encoder-emb-dim 8 \
--encoder-units-rnn 32 --encoder-layers-rnn 1 \
--encoder-units-mlp 16 --encoder-activation-mlp 'relu' \
--decoder 'cond-log-normal-mixture' \
--decoder-units-mlp 16 --decoder-units-mlp 16 \
--decoder-n-mixture 32 \
--decoder-n-mark-mixture 1 \
--decoder-encoding 'log_times_only' \
--decoder-hist-time-grouping 'concatenation' 
done 
"